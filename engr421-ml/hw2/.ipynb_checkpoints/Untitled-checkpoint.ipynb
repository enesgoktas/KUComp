{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aa7cf65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25 0.25 0.25 0.25]\n",
      "[array([ -7.48177328, -43.30108951]), array([-32.54451927,   3.0991768 ]), array([-0.98957165, 37.36564372]), array([41.0158642 ,  2.83626898])]\n",
      "[array([[596.51446769, -97.38122298],\n",
      "       [-97.38122298,  70.12428438]]), array([[ 229.06267779, -106.82064316],\n",
      "       [-106.82064316,  422.15817151]]), array([[436.85397986, 148.60965854],\n",
      "       [148.60965854, 322.83954932]]), array([[ 248.06205823, -175.62475341],\n",
      "       [-175.62475341,  379.20473545]])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (4000,2) and (4000,2) not aligned: 2 (dim 1) != 4000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vf/7jgkhkls7rx2qt9hlhj830hr0000gn/T/ipykernel_60712/2814940367.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m scores_train = calculate_score_values(X_train, sample_means,\n\u001b[0m\u001b[1;32m     88\u001b[0m                                       sample_covariances, class_priors)\n\u001b[1;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vf/7jgkhkls7rx2qt9hlhj830hr0000gn/T/ipykernel_60712/2814940367.py\u001b[0m in \u001b[0;36mcalculate_score_values\u001b[0;34m(X, class_means, class_covariances, class_priors)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Calculate the term inside the exponential function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mterm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_covariances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mterm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msample_means\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_covariances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msample_means\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mlog_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mterm2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_likelihood\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_priors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4000,2) and (4000,2) not aligned: 2 (dim 1) != 4000 (dim 0)"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.linalg as linalg\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "X_train = np.genfromtxt(fname = \"hw02_data_points.csv\", delimiter = \",\", dtype = float)\n",
    "y_train = np.genfromtxt(fname = \"hw02_class_labels.csv\", delimiter = \",\", dtype = int)\n",
    "\n",
    "\n",
    "\n",
    "# STEP 3\n",
    "# assuming that there are K classes\n",
    "# should return a numpy array with shape (K,)\n",
    "def estimate_prior_probabilities(y):\n",
    "    # your implementation starts below\n",
    "    unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "    total_samples = len(y)\n",
    "    class_priors = class_counts / total_samples\n",
    "    \n",
    "    # your implementation ends above\n",
    "    return(class_priors)\n",
    "\n",
    "class_priors = estimate_prior_probabilities(y_train)\n",
    "print(class_priors)\n",
    "\n",
    "\n",
    "\n",
    "# STEP 4\n",
    "# assuming that there are K classes and D features\n",
    "# should return a numpy array with shape (K, D)\n",
    "def estimate_class_means(X, y):\n",
    "    # your implementation starts below\n",
    "    unique_classes = np.unique(y)\n",
    "    sample_means = []\n",
    "    for c in unique_classes:\n",
    "        sample_means.append(np.mean(X[y == c], axis=0))\n",
    "    # your implementation ends above\n",
    "    return(sample_means)\n",
    "\n",
    "sample_means = estimate_class_means(X_train, y_train)\n",
    "print(sample_means)\n",
    "\n",
    "\n",
    "\n",
    "# STEP 5\n",
    "# assuming that there are K classes and D features\n",
    "# should return a numpy array with shape (K, D, D)\n",
    "def estimate_class_covariances(X, y):\n",
    "    # your implementation starts below\n",
    "    unique_classes = np.unique(y)\n",
    "    sample_covariances = []\n",
    "    for c in unique_classes:\n",
    "        class_samples = X[y == c]\n",
    "        sample_covariance = np.cov(class_samples, rowvar=False)\n",
    "        sample_covariances.append(sample_covariance)\n",
    "    # your implementation ends above\n",
    "    return(sample_covariances)\n",
    "\n",
    "sample_covariances = estimate_class_covariances(X_train, y_train)\n",
    "print(sample_covariances)\n",
    "\n",
    "\n",
    "\n",
    "# STEP 6\n",
    "# assuming that there are N data points and K classes\n",
    "# should return a numpy array with shape (N, K)\n",
    "def calculate_score_values(X, class_means, class_covariances, class_priors):\n",
    "    # your implementation starts below\n",
    "    num_classes = len(class_means)\n",
    "    num_samples = X.shape[0]\n",
    "    score_values = np.zeros((num_samples, num_classes))\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        # Calculate the term inside the exponential function\n",
    "        term1 = -2 * np.log(2 * np.pi) - 0.5 * np.log(np.linalg.det(class_covariances[c]))\n",
    "        term2 = np.sum(np.dot((X - class_means[c]), np.linalg.inv(class_covariances[c])) * (X - class_means[c]), axis=1)\n",
    "        log_likelihood = term1 - term2\n",
    "        score_values[:, c] = log_likelihood + np.log(class_priors[c])\n",
    "    \n",
    "    # your implementation ends above\n",
    "    return(score_values)\n",
    "\n",
    "scores_train = calculate_score_values(X_train, sample_means,\n",
    "                                      sample_covariances, class_priors)\n",
    "print(scores_train)\n",
    "\n",
    "\n",
    "\n",
    "# STEP 7\n",
    "# assuming that there are K classes\n",
    "# should return a numpy array with shape (K, K)\n",
    "def calculate_confusion_matrix(y_truth, scores):\n",
    "    # your implementation starts below\n",
    "    num_classes = scores.shape[1]\n",
    "    predicted_labels = np.argmax(scores, axis=1) + 1  \n",
    "    confusion_matrix = np.zeros((num_classes, num_classes))\n",
    "\n",
    "    for true_label, predicted_label in zip(y_truth, predicted_labels):\n",
    "        confusion_matrix[true_label - 1, predicted_label - 1] += 1  \n",
    "\n",
    "    # your implementation ends above\n",
    "    return(confusion_matrix)\n",
    "\n",
    "confusion_train = calculate_confusion_matrix(y_train, scores_train)\n",
    "print(confusion_train)\n",
    "\n",
    "\n",
    "\n",
    "def draw_classification_result(X, y, class_means, class_covariances, class_priors):\n",
    "    class_colors = np.array([\"#1f78b4\", \"#33a02c\", \"#e31a1c\", \"#6a3d9a\"])\n",
    "    K = np.max(y)\n",
    "\n",
    "    x1_interval = np.linspace(-75, +75, 151)\n",
    "    x2_interval = np.linspace(-75, +75, 151)\n",
    "    x1_grid, x2_grid = np.meshgrid(x1_interval, x2_interval)\n",
    "    X_grid = np.vstack((x1_grid.flatten(), x2_grid.flatten())).T\n",
    "    scores_grid = calculate_score_values(X_grid, class_means, class_covariances, class_priors)\n",
    "\n",
    "    score_values = np.zeros((len(x1_interval), len(x2_interval), K))\n",
    "    for c in range(K):\n",
    "        score_values[:,:,c] = scores_grid[:, c].reshape((len(x1_interval), len(x2_interval)))\n",
    "\n",
    "    L = np.argmax(score_values, axis = 2)\n",
    "\n",
    "    fig = plt.figure(figsize = (6, 6))\n",
    "    for c in range(K):\n",
    "        plt.plot(x1_grid[L == c], x2_grid[L == c], \"s\", markersize = 2, markerfacecolor = class_colors[c], alpha = 0.25, markeredgecolor = class_colors[c])\n",
    "    for c in range(K):\n",
    "        plt.plot(X[y == (c + 1), 0], X[y == (c + 1), 1], \".\", markersize = 4, markerfacecolor = class_colors[c], markeredgecolor = class_colors[c])\n",
    "    plt.xlim((-75, 75))\n",
    "    plt.ylim((-75, 75))\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.show()\n",
    "    return(fig)\n",
    "    \n",
    "fig = draw_classification_result(X_train, y_train, sample_means, sample_covariances, class_priors)\n",
    "fig.savefig(\"hw02_result_different_covariances.pdf\", bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "\n",
    "# STEP 8\n",
    "# assuming that there are K classes and D features\n",
    "# should return a numpy array with shape (K, D, D)\n",
    "def estimate_shared_class_covariance(X, y):\n",
    "    # your implementation starts below\n",
    "    shared_covariance = np.cov(X, rowvar=False)\n",
    "    return shared_covariance\n",
    "    # your implementation ends above\n",
    "    return(sample_covariances)\n",
    "\n",
    "sample_covariances = estimate_shared_class_covariance(X_train, y_train)\n",
    "print(sample_covariances)\n",
    "\n",
    "scores_train = calculate_score_values(X_train, sample_means,\n",
    "                                      sample_covariances, class_priors)\n",
    "print(scores_train)\n",
    "\n",
    "confusion_train = calculate_confusion_matrix(y_train, scores_train)\n",
    "print(confusion_train)\n",
    "\n",
    "fig = draw_classification_result(X_train, y_train, sample_means, sample_covariances, class_priors)\n",
    "fig.savefig(\"hw02_result_shared_covariance.pdf\", bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c499d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
